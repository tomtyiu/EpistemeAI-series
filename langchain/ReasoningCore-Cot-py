# %pip install langchain
# %pip install transformers
# %pip install huggingface_hub

from langchain_community.llms import HuggingFaceEndpoint
from langchain.prompts import PromptTemplate

from langchain_community.llms import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

#Importing required Packages

from langchain_community.llms import HuggingFaceEndpoint
from langchain.prompts import PromptTemplate

#Writing Prompt as Text String

prompt = f"""
        {system_prompt}

        You are an AI assistant that uses a Chain of Thought (CoT) approach with reflection to answer queries. Follow these steps:

        1. Think through the problem step by step within the <thinking> tags.
        2. Reflect on your thinking to check for any errors or improvements within the <reflection> tags.
        3. Make any necessary adjustments based on your reflection.
        4. Provide your final, concise answer within the <output> tags.

        Important: The <thinking> and <reflection> sections are for your internal reasoning process only. 
        Do not include any part of the final answer in these sections. 
        The actual response to the query must be entirely contained within the <output> tags.

        Use the following format for your response:
        <thinking>
        [Your step-by-step reasoning goes here. This is your internal thought process, not the final answer.]
        <reflection>
        [Your reflection on your reasoning, checking for errors or improvements]
        </reflection>
        [Any adjustments to your thinking based on your reflection]
        </thinking>
        <output>
        [Your final, concise answer to the query. This is the only part that will be shown to the user.]
        </output>
        """

#Creating Prompt Template

question_template = PromptTemplate(
    input_variables=["uestion"],
    template= prompt)
              
#Having Sneak Peak into crafted Prompt Template

sneakpeak = question_template.format(question="who is Elon Musk")


print(sneakpeak)

#Setting Up LLM

repo_id = "import transformers
import torch
from langchain_community.llms import HuggingFaceEndpoint
from langchain_community.chat_models.huggingface import ChatHuggingFace

from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="float16",
    bnb_4bit_use_double_quant=True,
)

model_id = "EpistemeAI/ReasoningCore-3B-0"
pipeline = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={"quantization_config": quantization_config}, #for fast response. For full 16bit inference, remove this code.
    device_map="auto",
)
messages = [
    {"role": "system", "content":  """
    Environment: ipython. Tools: brave_search, wolfram_alpha. Cutting Knowledge Date: December 2023. Today Date: 4 October 2024\n
    You are a coding assistant with expert with everything\n
    Ensure any code you provide can be executed \n
    with all required imports and variables defined. List the imports.  Structure your answer with a description of the code solution. \n
    write only the code. do not print anything else.\n
    debug code if error occurs. \n
    ### Question: {}\n
    ### Answer: {} \n
    """},
    {"role": "user", "content": "Train an AI model to predict the number of purchases made per customer in a given store."}
]
outputs = pipeline(messages, max_new_tokens=128, do_sample=True, temperature=0.01, top_k=100, top_p=0.95)
print(outputs[0]["generated_text"][-1])
"


llm = HuggingFaceEndpoint(repo_id= repo_id, huggingfacehub_api_token = "Replace With Your Hugging Face API creatred above" )

#Invoking the Query


ans_meta = llm.invoke(prompt)

print(ans_meta)
